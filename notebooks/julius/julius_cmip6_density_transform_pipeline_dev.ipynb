{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "import intake\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cf_xarray\n",
    "import dask\n",
    "# dask.config.set({\"array.slicing.split_large_chunks\": True}) # avoid large chunks to be created.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fastjmd95 import rho\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from xarrayutils.file_handling import (\n",
    "    write,\n",
    "    maybe_create_folder,\n",
    "    file_exist_check,\n",
    "    temp_write_split,\n",
    ")\n",
    "from xarrayutils.utils import (\n",
    "    remove_bottom_values,\n",
    "    #mask_mixedlayer\n",
    ")\n",
    "from cmip6_preprocessing.preprocessing import (\n",
    "    combined_preprocessing\n",
    ")\n",
    "from cmip6_preprocessing.drift_removal import (\n",
    "    remove_trend,\n",
    "    match_and_remove_trend\n",
    ")\n",
    "from cmip6_preprocessing.utils import (\n",
    "    cmip6_dataset_id\n",
    ")\n",
    "\n",
    "from cmip6_preprocessing.postprocessing import (\n",
    "    combine_datasets,\n",
    "    concat_experiments,\n",
    "    match_metrics,\n",
    "    merge_variables,\n",
    "    interpolate_grid_label,\n",
    ")\n",
    "from cmip6_preprocessing.drift_removal import match_and_remove_trend\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from cmip6_omz.upstream_stash import (\n",
    "    transform_wrapper,\n",
    "    pick_first_member,\n",
    "    construct_static_dz\n",
    ")\n",
    "from cmip6_omz.omz_tools import (\n",
    "    omz_thickness,\n",
    "    sigma_bins,\n",
    "    align_missing,\n",
    "    preprocessing_wrapper,\n",
    "    vol_consistency_check_wrapper\n",
    ")\n",
    "\n",
    "from cmip6_omz.utils import (\n",
    "    cmip6_collection,\n",
    "    o2_models,\n",
    ")\n",
    "\n",
    "from cmip6_omz.plotting import plot_omz_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What adds to the tasks \n",
    "\n",
    "- detrending...might want to save out temp after?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x2b638163cb20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "from multiprocessing.pool import ThreadPool\n",
    "dask.config.set(pool=ThreadPool(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What I have done:\n",
    "- Remove all old refs to the other repos\n",
    "- Refactoring of the metrics matching\n",
    "- Using only the regridding to combine variables \n",
    "    - Need to patch in Norwegian models\n",
    "- Single cell for filtering/checking all datasets for required vars/metrics\n",
    "    - This also logs all the problems in one place\n",
    "\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Test with netcdf archive (or at least update the zarr?\n",
    "- Test performance with strip encoding?\n",
    "- [x] Try with new trends\n",
    "- Gotta fix the logic in the interpolate function to just merge variables that all have the same grid label (Nor ESM)\n",
    "- [x] **The damn norwegian models have no area...**\n",
    "- [x] Can I check each variable for the giant chunks after concatting?\n",
    "- [ ] CM4 age is chunked badly...\n",
    "- [ ] Figure out how to deal with the access data properly (thickness concat fails)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This could go upstream in a more general form\n",
    "## but for now let's keep it here and readable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should go to upstream_stash\n",
    "# def load_trend_dict(ds_dict, verbose = False):\n",
    "    \n",
    "#     path_jb = '/tigress/GEOCLIM/LRGROUP/jbusecke/projects/aguadv_omz_busecke_2021/data/processed/linear_regression_time_zarr_multimember'\n",
    "#     trendfolder = pathlib.Path(path_jb)\n",
    "#     trend_models = np.unique([ds.attrs['source_id'] for ds in ds_dict.values()])\n",
    "#     flist = []\n",
    "#     for tm in trend_models:\n",
    "#         flist = flist + list(trendfolder.glob(f'*{tm}*_trend.nc'))\n",
    "    \n",
    "#     total = len(flist)\n",
    "#     progress = progress_bar(range(total))\n",
    "    \n",
    "#     trend_dict = {}\n",
    "#     for i,path in enumerate(flist):\n",
    "#         key = path.stem\n",
    "#         ds = xr.open_mfdataset([path])\n",
    "#         # write the filename in the dataset\n",
    "#         ds.attrs.update({'filepath':str(path)})\n",
    "#         # exclude all nan slopes (why are these there in the first place?)\n",
    "#         if not np.isnan(ds.slope).all():\n",
    "#             trend_dict[key] = ds\n",
    "#         else:\n",
    "#             if verbose:\n",
    "#                 print(f\"found all nan regression data for {path}\")\n",
    "#         progress.update(i)\n",
    "#     progress.update(total)\n",
    "    \n",
    "#     return trend_dict\n",
    "\n",
    "\n",
    "# #These are fixes so that the trend data works with cmip6_pp match_and_remove_trend\n",
    "# #these issues should be addressed in the next iteration of trend file production\n",
    "# def fix_trend_metadata(trend_dict):\n",
    "#     for name, ds in trend_dict.items():\n",
    "#         #restore attributes to trend datasets using file names\n",
    "#         fn = (ds.attrs['filepath']).rsplit(\"/\")[-1]\n",
    "#         fn_parse = fn.split('_')\n",
    "#         ds.attrs['source_id'] = fn_parse[2]\n",
    "#         ds.attrs['grid_label'] = fn_parse[5]\n",
    "#         ds.attrs['experiment_id'] = fn_parse[3]\n",
    "#         ds.attrs['table_id'] = fn_parse[4]\n",
    "#         ds.attrs['variant_label'] = fn_parse[7]\n",
    "#         ds.attrs['variable_id'] = fn_parse[8]\n",
    "        \n",
    "#         #rename 'slope' variable to variable_id\n",
    "#         if \"slope\" in ds.variables:\n",
    "#             ds = ds.rename({\"slope\":ds.attrs[\"variable_id\"]})\n",
    "        \n",
    "#         #error was triggered in line 350 of cmip6_preprocessing.drift_removal\n",
    "#         ##this is a temporary workaround, and the one part of this function that might\n",
    "#         ##require an upstream fix (though it might just be an environment issue)\n",
    "#         ds = ds.drop('trend_time_range')\n",
    "        \n",
    "#         trend_dict[name] = ds\n",
    "        \n",
    "#     return trend_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import progress_bar\n",
    "from zarr.convenience import consolidate_metadata\n",
    "\n",
    "def append_write_zarr(ds, store, split_chunks, split_dim='time', consolidate=True):\n",
    "    \"\"\"Save a dataset with a loop to avoid blowing up complicated dask graphs\"\"\"\n",
    "    splits = list(range(0, len(ds[split_dim]), split_chunks))\n",
    "    splits.append(None)\n",
    "    datasets = []\n",
    "    for ii in range(len(splits)-1):\n",
    "        datasets.append(ds.isel({split_dim:slice(splits[ii], splits[ii+1])}))\n",
    "    \n",
    "    # .to_zarr needs that we write the first datasets without appending\n",
    "    datasets[0].to_zarr(store, mode='w')\n",
    "    for ds_sub in progress_bar(datasets[1:]):\n",
    "        ds_sub.to_zarr(store, mode='a', append_dim=split_dim)\n",
    "    \n",
    "    if consolidate:\n",
    "        consolidate_metadata(str(store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_yearly(ds_in, freq=\"1AS\"):\n",
    "    # this drops some coordinates, so i need to convert them to data_vars and then reconvert\n",
    "    time_coords = [\n",
    "        co\n",
    "        for co in list(ds_in.coords)\n",
    "        if \"time\" in ds_in[co].dims and co not in [\"time\", \"time_bounds\"]\n",
    "    ]\n",
    "    ds_out = ds_in.reset_coords(time_coords).coarsen(time=12).mean()\n",
    "    ds_out = ds_out.assign_coords({co: ds_out[co] for co in time_coords})\n",
    "    ds_out.attrs.update({k: v for k, v in ds_in.attrs.items() if k not in [\"table_id\"]})\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_zarr(fn):\n",
    "    extension = fn.split('.')[-1]\n",
    "    if extension == 'nc':\n",
    "        is_zarr = False\n",
    "    elif extension == 'zarr':\n",
    "        is_zarr = True\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized File Extension')\n",
    "    return is_zarr\n",
    "\n",
    "def reload_preexisting(filename, overwrite = True):\n",
    "    print(\"Skipping. File exists already.\")\n",
    "    if is_zarr(filename):\n",
    "        ds_sigma_reloaded = xr.open_zarr(\n",
    "            filename, use_cftime=True, consolidated=True\n",
    "        )\n",
    "    else:\n",
    "        ds_sigma_reloaded = xr.open_dataset(\n",
    "            filename, use_cftime = True\n",
    "        )\n",
    "        try:\n",
    "            plot_omz_results(ds_sigma_reloaded)\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting failed with: {e}\")\n",
    "    return ds_sigma_reloaded\n",
    "    \n",
    "def strip_encoding(ds):\n",
    "    \"\"\"Strips the encoding from xr.dataset... This seems like a bug to me.\"\"\"\n",
    "    for var in ds.variables:\n",
    "        ds[var].encoding = {}\n",
    "    ds.encoding = {}\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local convenience functions for final cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs2/jbusecke/conda_tigressdata/envs/cmip6_omz/lib/python3.9/site-packages/xarrayutils/file_handling.py:118: UserWarning: Folder ../../data/processed/fine_density_tests_combined_2 does already exist.\n",
      "  warnings.warn(f\"Folder {path} does already exist.\", UserWarning)\n",
      "/scratch/gpfs2/jbusecke/conda_tigressdata/envs/cmip6_omz/lib/python3.9/site-packages/xarrayutils/file_handling.py:118: UserWarning: Folder ../../data/temp/scratch_temp/fine_density_tests_combined_2 does already exist.\n",
      "  warnings.warn(f\"Folder {path} does already exist.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "foldername = \"fine_density_tests_combined_2\"\n",
    "# ofolder = maybe_create_folder(f\"../../data/external/{foldername}\")\n",
    "ofolder = maybe_create_folder(f\"../../data/processed/{foldername}\")\n",
    "tempfolder = maybe_create_folder(f\"../../data/temp/scratch_temp/{foldername}\")\n",
    "\n",
    "# global parameters\n",
    "o2_bins = np.array([10, 40, 60, 80, 100, 120])\n",
    "fine_sigma_bins = sigma_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(cmip6_collection(zarr=True)) #TODO: Check with nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CanESM5-CanOE',\n",
       " 'CanESM5',\n",
       " 'CNRM-ESM2-1',\n",
       " 'ACCESS-ESM1-5',\n",
       " 'MPI-ESM-1-2-HAM',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'MIROC-ES2L',\n",
       " 'UKESM1-0-LL',\n",
       " 'MPI-ESM1-2-HR',\n",
       " 'MPI-ESM1-2-LR',\n",
       " 'MRI-ESM2-0',\n",
       " 'NorCPM1',\n",
       " 'NorESM1-F',\n",
       " 'NorESM2-LM',\n",
       " 'NorESM2-MM',\n",
       " 'GFDL-CM4',\n",
       " 'GFDL-ESM4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.version.zstore'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='90' class='' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [90/90 00:24<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.version.zstore'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='41' class='' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [41/41 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if this does not work on jupyter.rc, we can add some logic to \n",
    "col = intake.open_esm_datastore(cmip6_collection(zarr=True)) #TODO: Check with nc files\n",
    "\n",
    "z_kwargs={\"decode_times\": True, \"use_cftime\": True, \"consolidated\": True}\n",
    "n_kwargs={\"decode_times\": True, \"use_cftime\": True, 'chunks':{'time':3}}\n",
    "\n",
    "variable_ids = [\"thetao\", \"so\", \"o2\", \"agessc\"] #\"mlotst\"\n",
    "metric_variable_ids = [\"thkcello\", \"areacello\"] #\"mlotst\"\n",
    "\n",
    "# models = o2_models()\n",
    "# models = ['GFDL-ESM4', 'GFDL-CM4', 'ACCESS-ESM1-5']#`,# # shorter test run....,\n",
    "# models = [m for m in o2_models() if 'GFDL-ESM4' in m or 'Nor' in m]\n",
    "# models = [m for m in o2_models() if ('ACCESS' not in m and 'GFDL' not in m and 'HR' not in m)]\n",
    "models = [\n",
    "#     'MPI-ESM1-2-HR',\n",
    "    'MRI-ESM2-0',\n",
    "    'NorESM2-LM',\n",
    "#     'GFDL-CM4',\n",
    "    'GFDL-ESM4',\n",
    "]\n",
    "\n",
    "cat = col.search(\n",
    "    source_id = models,\n",
    "    grid_label=[\"gr\", \"gn\"],\n",
    "    experiment_id=[\"historical\", \"ssp585\"],\n",
    "    table_id=[\"Omon\"],\n",
    "    variable_id=variable_ids,\n",
    ")\n",
    "ds_dict = cat.to_dataset_dict(\n",
    "        aggregate=False,\n",
    "        zarr_kwargs=z_kwargs,\n",
    "        cdf_kwargs=n_kwargs,\n",
    "        preprocess=combined_preprocessing,\n",
    "    )\n",
    "\n",
    "# make a separate metric dict to catch all possible metrics!\n",
    "cat_metrics = col.search(source_id=models,variable_id=metric_variable_ids)\n",
    "ds_metric_dict = cat_metrics.to_dataset_dict(\n",
    "        aggregate=False,\n",
    "        zarr_kwargs=z_kwargs,\n",
    "        cdf_kwargs=n_kwargs,\n",
    "        preprocess=combined_preprocessing,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CNRM-ESM2-1', 'UKESM1-0-LL', 'GFDL-ESM4', 'GFDL-CM4', 'CanESM5',\n",
       "       'CanESM5-CanOE', 'MPI-ESM1-2-HR', 'ACCESS-ESM1-5', 'MRI-ESM2-0',\n",
       "       'MIROC-ES2L', 'IPSL-CM6A-LR', 'NorESM2-LM', 'NorESM2-MM',\n",
       "       'MPI-ESM1-2-LR', 'MPI-ESM-1-2-HAM', 'NorESM1-F', 'NorCPM1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.df['source_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rechunk the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk(ds):\n",
    "    if 'time' in ds.dims:\n",
    "        return ds.chunk({'time':1})\n",
    "    else:\n",
    "        return ds\n",
    "\n",
    "ds_dict = {k: rechunk(ds) for k,ds in ds_dict.items()}\n",
    "ds_metric_dict = {k: rechunk(ds) for k,ds in ds_metric_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='510' class='' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [510/510 00:31<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# new files (change in later and get rid of `load_trend_dict` (or refactor?) and `fix_trend_metadata`)\n",
    "# Load all trend files\n",
    "flist = list(pathlib.Path('../../data/external/cmip6_control_drifts/').absolute().glob('*.nc'))\n",
    "flist = [f for f in flist if any([v in str(f) for v in variable_ids])]\n",
    "trend_dict = {}\n",
    "for f in progress_bar(flist):\n",
    "    trend_dict[f.stem] = xr.open_mfdataset([f])\n",
    "#     trend_dict[f.stem] = xr.open_dataset(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these ones are messed up...need a better way to deal with that in the previous step\n",
    "# see https://github.com/jbusecke/cmip6_preprocessing/issues/175\n",
    "incomplete_keys = ['CMIP.IPSL.IPSL-CM6A-LR.historical.r3i1p1f1.Omon.gn.none.area_o2']\n",
    "trend_dict = {k:ds for k,ds in trend_dict.items() if k not in incomplete_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbusecke/code/cmip6_preprocessing/cmip6_preprocessing/postprocessing.py:60: UserWarning: Could not find a matching dataset for ScenarioMIP.NCC.NorESM2-LM.ssp585.r1i1p1f1.Omon.gn.none.agessc\n",
      "  warnings.warn(nomatch_msg)\n",
      "/home/jbusecke/code/cmip6_preprocessing/cmip6_preprocessing/postprocessing.py:60: UserWarning: Could not find a matching dataset for ScenarioMIP.NCC.NorESM2-LM.ssp585.r1i1p1f1.Omon.gn.none.so\n",
      "  warnings.warn(nomatch_msg)\n",
      "/home/jbusecke/code/cmip6_preprocessing/cmip6_preprocessing/postprocessing.py:60: UserWarning: Could not find a matching dataset for ScenarioMIP.NCC.NorESM2-LM.ssp585.r1i1p1f1.Omon.gn.none.thetao\n",
      "  warnings.warn(nomatch_msg)\n"
     ]
    }
   ],
   "source": [
    "ddict_tracers_detrended = match_and_remove_trend(\n",
    "    ds_dict,\n",
    "    trend_dict,\n",
    "#     check_mask=False\n",
    ")\n",
    "# print('THIS IS DANGEROUS. CHECK THE MASKS!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match metrics (there are still quite a few missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one causes problems because the time is not as long as the full data...\n",
    "problem_keys = [\n",
    "    #shorter run? Missing beginning?\n",
    "    'CMIP.CNRM-CERFACS.CNRM-ESM2-1.historical.r6i1p1f2.Omon.so.gn.v20200117./projects/GEOCLIM/LRGROUP/jbusecke/projects/cmip_data_management_princeton/builder/../zarr_conversion/CMIP6/CMIP/CNRM-CERFACS/CNRM-ESM2-1/historical/r6i1p1f2/Omon/so/gn/v20200117/CMIP.CNRM-CERFACS.CNRM-ESM2-1.historical.r6i1p1f2.Omon.so.gn.v20200117.zarr'\n",
    "]\n",
    "# ddict_tracers_detrended_filtered = {k:ds.squeeze() for k, ds in ddict_tracers_detrended.items() if k not in problem_keys}\n",
    "ddict_tracers_detrended_filtered = {k:ds.squeeze() for k, ds in ddict_tracers_detrended.items() if not (\"CNRM-ESM2-1\" in k and \"r6i1p1f2\" in k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('matching metrics\\n')\n",
    "ddict_matched = match_metrics(ddict_tracers_detrended_filtered, ds_metric_dict, ['areacello', 'thkcello'], print_statistics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do I need to rechunk here for the high res models? I am currently doing this for CM4 and ESM4, but I might have to adjust the source data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('interpolate grids\\n')\n",
    "ddict_matched_regrid = interpolate_grid_label(ddict_matched, merge_kwargs={'compat':'override'}) # This should be a default soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch the norwegian model in manually\n",
    "ddict_patch = merge_variables(ddict_matched)\n",
    "for name, ds in ddict_patch.items():\n",
    "    if 'Nor' in name and 'gr' in name:\n",
    "        patch_name = name.replace('.gr','')\n",
    "        ddict_matched_regrid[patch_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(list(ddict_matched_regrid.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate experiments and pick the first full one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# somehow xarray cannot deal with comparing list/int attrs (Occurs in CM4)\n",
    "# I should raise that, but lets fix it quickly here\n",
    "def clean_attrs(ds):\n",
    "    for a, attr in ds.attrs.items():\n",
    "        if isinstance(attr, int):\n",
    "            ds.attrs[a] = [attr]\n",
    "    return ds\n",
    "\n",
    "ddict_matched_regrid = {k:clean_attrs(ds) for k, ds in ddict_matched_regrid.items()}\n",
    "\n",
    "ddict_ex_combined = concat_experiments(\n",
    "    ddict_matched_regrid,\n",
    "    concat_kwargs={\n",
    "        'combine_attrs': 'drop_conflicts',\n",
    "        'compat': 'override',\n",
    "        'coords': 'minimal'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quick fix for inhomogenous metrics\n",
    "I have to think about this more. So basically some of the models (ACCESS) have time variables thickness for ssp585 and static for the historical.\n",
    "This leads to huge dask chunks. For now I am taking those out, which will lead to a static recompute later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_chunks(ds):\n",
    "    trigger_vars = []\n",
    "    for var in ds.variables:\n",
    "        if isinstance(ds[var].data, dask.array.Array):\n",
    "            for di, ch in zip(ds[var].dims, ds[var].data.chunks):\n",
    "                if di == 'time':\n",
    "                    if any([c>10 for c in list(ch)]):\n",
    "                        trigger_vars.append(var)\n",
    "                    \n",
    "    return trigger_vars\n",
    "\n",
    "# drop the variables in question\n",
    "ddict_ex_combined_filtered = {}\n",
    "for name,ds in ddict_ex_combined.items():\n",
    "    check = check_chunks(ds)\n",
    "    if len(check)>0:\n",
    "        print(name)\n",
    "        print(check)\n",
    "    ds = ds.drop(check)\n",
    "    ddict_ex_combined_filtered[name] = ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outstanding issue ACCESS cant combine with some having no thkness\n",
    "So basically in this example:\n",
    "```python\n",
    "ds1 = ddict_matched_regrid['ACCESS-ESM1-5.historical.Omon.r1i1p1f1']\n",
    "ds2 = ddict_matched_regrid['ACCESS-ESM1-5.ssp585.Omon.r1i1p1f1']\n",
    "ds2\n",
    "\n",
    "ds_combined = xr.concat([ds1.drop('thkcello'), ds2], 'time', **{'combine_attrs': 'drop_conflicts', 'compat': 'override', 'coords': 'minimal'})\n",
    "ds_combined\n",
    "```\n",
    "I figured that the thkcello should be dropped, but xarray fails. Raise an issue about that. Otherwise Ill have to check in the combination function...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only pick full runs (historical and ssp585)\n",
    "ddict_ex_combined_full = {k:ds for k,ds in ddict_ex_combined_filtered.items() if len(ds.time)>3000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_ex_combined_full.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check datasets for completeness and log the ones with problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cmip6_preprocessing.grids import combine_staggered_grid\n",
    "problems = {'missing_variables':[], 'missing_area':[], 'missing_thickness':[], 'reconstructed_area':[], 'reconstructed_thickness':[]}\n",
    "ddict_filtered = {}\n",
    "for name, ds in ddict_ex_combined_full.items():\n",
    "    flag = False\n",
    "    # Check that all necessary variables are given\n",
    "    missing_variables = [va for va in [\"thetao\", \"so\", \"o2\"] if va not in ds.variables]\n",
    "    if len(missing_variables)>0:\n",
    "        flag = True\n",
    "        problems['missing_variables'].append((name, missing_variables))\n",
    "        \n",
    "    # Check for area\n",
    "    if not 'areacello' in ds.coords:\n",
    "        if ds.attrs['grid_label'] == 'gr': # only reconstruct for regular grids\n",
    "            grid, ds = combine_staggered_grid(ds, recalculate_metrics=True)\n",
    "            # I am dropping dz_t here so it can be uniformly reconstructed\n",
    "            ds = ds.drop('dz_t')\n",
    "            ds = ds.assign_coords(areacello = (ds.dx_t * ds.dy_t).reset_coords(drop=True))\n",
    "            problems['reconstructed_area'].append(name)\n",
    "            assert 'areacello' in ds.coords\n",
    "        else:\n",
    "            flag = True\n",
    "            problems['missing_area'].append(name)\n",
    "    \n",
    "    # Check for thickness (and rename) TODO: We should probably not rename and just refactor to use `thkcello`\n",
    "    if \"thkcello\" in ds.coords:\n",
    "        ds = ds.rename({'thkcello': 'dz_t'})\n",
    "    else:\n",
    "        # try to reconstruct the thickness from static info\n",
    "        try:\n",
    "#             lev_vertices = cf_xarray.bounds_to_vertices(ds.lev_bounds, 'bnds').load()\n",
    "#             dz_t = lev_vertices.diff('lev_vertices')\n",
    "#             ds = ds.assign_coords(dz_t=('lev', dz_t.data))\n",
    "            ds = construct_static_dz(ds).rename({'thkcello': 'dz_t'})\n",
    "            problems['reconstructed_thickness'].append(name)\n",
    "        except Exception as e:\n",
    "            print(f'{name} thickness reconstruction failed with {e}')\n",
    "            print(ds)\n",
    "            problems['missing_thickness'].append(name)\n",
    "            flag=True\n",
    "            \n",
    "    if not flag:\n",
    "        ddict_filtered[name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.sort(list(ddict_filtered.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_final = pick_first_member(ddict_filtered)#\n",
    "list(np.sort(list(ddict_final.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hacking time ðŸ˜Ž\n",
    "\n",
    "Not sure if this actually improved things...but it reduces the number of tasks...which is generally good.\n",
    "\n",
    "Bring this over to xarrayutils (more info/test in `dev_efficient_bottom_removal`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just code that shit in numba\n",
    "from numba import float64, guvectorize\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "@guvectorize(\n",
    "    [\n",
    "        (float64[:], float64[:]),\n",
    "    ],\n",
    "    \"(n)->(n)\",\n",
    "    nopython=True,\n",
    ")\n",
    "def _remove_last_value(data, output):\n",
    "    # initialize output\n",
    "    output[:] = data[:]\n",
    "    for i in range(len(data)-1):\n",
    "        if np.isnan(output[i+1]):\n",
    "            output[i] = np.nan\n",
    "    # take care of boundaries\n",
    "    if not np.isnan(output[-1]):\n",
    "        output[-1] = np.nan\n",
    "\n",
    "def remove_bottom_values_numba(da, dim='lev'):\n",
    "    \n",
    "    out = xr.apply_ufunc(\n",
    "        _remove_last_value,\n",
    "        da,\n",
    "        input_core_dims=[[dim]],\n",
    "        output_core_dims=[[dim]],\n",
    "        dask=\"parallelized\",\n",
    "        output_dtypes=[da.dtype],\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def remove_bottom_values_recoded(ds, dim=\"lev\", fill_val=-1e10):\n",
    "    \"\"\"Remove the deepest values that are not nan along the dimension `dim`\"\"\"\n",
    "    # for now assume that values of `dim` increase along the dimension\n",
    "    if ds[dim][0] > ds[dim][-1]:\n",
    "        raise ValueError(\n",
    "            f\"It seems like `{dim}` has decreasing values. This is not supported yet. Please sort before.\"\n",
    "        )\n",
    "    else:\n",
    "        ds_masked = xr.Dataset({va:remove_bottom_values_numba(ds[va]) for va in ds.data_vars})\n",
    "        ds_masked = ds_masked.transpose(*tuple([di for di in ds.dims if di in ds_masked]))\n",
    "        ds_masked = ds_masked.assign_coords({co:ds[co].transpose(*[di for di in ds.dims if di in ds[co]]) for co in ds.coords})\n",
    "        ds_masked.attrs = ds.attrs\n",
    "        ds_masked = ds_masked\n",
    "        return ds_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final loop to vertiLocalClustery transform to sigma-space and save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_omz.omz_tools import omz_thickness_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "def print_html(ds):\n",
    "    display(HTML(ds._repr_html_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will have to process the control runs seperately\n",
    "#         if ds.attrs[\"experiment_id\"] == \"piControl\":\n",
    "#             ds = ds.isel(time=slice(-300 * 12, None))\n",
    "\n",
    "\n",
    "\n",
    "# overwrite = True\n",
    "overwrite = False\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')# might need to remove later...\n",
    "    for synthetic in [True, False]:\n",
    "        for mi, (name, ds) in enumerate(ddict_final.items()):\n",
    "            \n",
    "            t0 = time.time()\n",
    "            synthetic_string = 'synthetic example' if synthetic else ' '\n",
    "            print(f\"######################{name} {synthetic_string} ({mi+1}/{len(ddict_filtered)}) ###############\")\n",
    "\n",
    "            dataset_id = f\"{cmip6_dataset_id(ds)}\"\n",
    "\n",
    "            if synthetic:\n",
    "                filename = ofolder.joinpath(f\"{dataset_id}_synthetic.zarr\")\n",
    "            else:\n",
    "                filename = ofolder.joinpath(f\"{dataset_id}.zarr\")\n",
    "\n",
    "\n",
    "            if file_exist_check(filename) and not overwrite:\n",
    "                ds_sigma_reloaded = reload_preexisting(str(filename))\n",
    "            else:\n",
    "                print(f\"Writing to {filename}\")\n",
    "                tempfilelist = []\n",
    "                \n",
    "                ds = preprocessing_wrapper(ds)\n",
    "                \n",
    "                # clean up the chunk encoding (can probably be dropped in newer xarray versions but leave for now)\n",
    "                ds = strip_encoding(ds)\n",
    "                \n",
    "                # I need to align.mask the thickness aswell!\n",
    "                ds = ds.reset_coords([\"dz_t\"])\n",
    "                #perform nan-masking functions\n",
    "                ds = align_missing(ds)\n",
    "    #             ds = remove_bottom_values(ds)\n",
    "                ds = remove_bottom_values_recoded(ds)\n",
    "                ds = ds.set_coords(\"dz_t\")\n",
    "\n",
    "                # reconstruct the potential density\n",
    "                ds[\"sigma_0\"] = (rho(ds.so, ds.thetao, 0) - 1000)\n",
    "                \n",
    "                # If active create synthetic control dataset with constant historical o2\n",
    "\n",
    "                if synthetic:\n",
    "                    with ProgressBar():\n",
    "                        o2_hist = ds.o2.sel(time=slice('1850', '1900')).mean('time').load()\n",
    "                    o2_hist_broadcasted = xr.ones_like(ds.sigma_0) * o2_hist\n",
    "                    ds = ds.assign(o2=o2_hist_broadcasted)\n",
    "\n",
    "                    assert np.allclose(ds.o2.isel(time=0).load(), ds.o2.isel(time=-100).load(), equal_nan=True)\n",
    "                    assert not np.allclose(ds.sigma_0.isel(time=0), ds.sigma_0.isel(time=-100), equal_nan=True)\n",
    "                \n",
    "\n",
    "                o2_bin_chunks=-1\n",
    "                \n",
    "                if 'GFDL' in name or 'HR' in name:\n",
    "                    #################################################################\n",
    "                    # rechunk the high res models here, they always crash otherwise #\n",
    "                    #################################################################\n",
    "#                     # age is messed up in CM4/ESM4 drop that for now\n",
    "#                     if 'agessc' in ds.data_vars:\n",
    "#                         ds = ds.drop(['agessc'])\n",
    "                    \n",
    "                    # also set some other parameters\n",
    "                    o2_bin_chunks = 1\n",
    "                    \n",
    "                    print(f\"Temp saving to\")\n",
    "                    with ProgressBar():\n",
    "                        ds_reloaded, tempfilelist_var = temp_write_split(\n",
    "                            ds,\n",
    "                            tempfolder.joinpath(f\"{name}_rechunked\"),\n",
    "                            verbose=False,\n",
    "    #                         method='variables',\n",
    "                            method='dimension',\n",
    "                            split_interval=24,\n",
    "                        )\n",
    "                        tempfilelist.extend(tempfilelist_var)\n",
    "                    print_html(ds_reloaded)\n",
    "                    ds = ds_reloaded\n",
    "                \n",
    "\n",
    "                    \n",
    "                ds[\"omz_thickness\"] = omz_thickness_efficient(\n",
    "                    ds, o2_bins=o2_bins, bin_chunks=o2_bin_chunks\n",
    "                )\n",
    "        \n",
    "#                 print_html(ds)\n",
    "\n",
    "                ds_sigma_monthly = transform_wrapper(ds, sigma_bins=fine_sigma_bins)\n",
    "                \n",
    "#                 print_html(ds_sigma_monthly)\n",
    "\n",
    "                # Check that the total ocean volume has not changed in the transformation\n",
    "                assert vol_consistency_check_wrapper(ds, ds_sigma_monthly)\n",
    "\n",
    "                # average yearly (otherwise the outputs become huuuuge)\n",
    "                ds_sigma_yearly = resample_yearly(ds_sigma_monthly)\n",
    "                    \n",
    "#                     ds_sigma_yearly_reloaded, tempfilelist_var = temp_write_split(\n",
    "#                         ds_sigma_yearly,\n",
    "#                         tempfolder,\n",
    "#                         verbose=False,\n",
    "#                         method='dimension',\n",
    "#                         split_interval=1 if len(ds.x)>400 else 10,\n",
    "#                     )\n",
    "#                 tempfilelist.extend(tempfilelist_var)\n",
    "\n",
    "                #################### write out results ########################\n",
    "#                 ds_sigma_reloaded = write(\n",
    "#                     ds_sigma_yearly_reloaded,\n",
    "#                     filename,\n",
    "#                     overwrite=False,\n",
    "#                     force_load=False,\n",
    "#                     check_zarr_complete=True,\n",
    "#                 )\n",
    "\n",
    "                dim_split = 5\n",
    "                if len(ds.x)> 400:\n",
    "                    dim_split = 2\n",
    "                if len(ds.x)>1000:\n",
    "                    dim_split = 1\n",
    "                \n",
    "                with ProgressBar():\n",
    "                    append_write_zarr(ds_sigma_yearly, filename, 10)\n",
    "                \n",
    "                ds_sigma_reloaded = xr.open_zarr(\n",
    "                    filename,\n",
    "                    consolidated=True,\n",
    "                    use_cftime=True\n",
    "                )\n",
    "\n",
    "                ###### delete temps ######\n",
    "                print('removing temps')\n",
    "                for tf in tempfilelist:\n",
    "                    if tf.exists():\n",
    "                        shutil.rmtree(tf)\n",
    "\n",
    "                        \n",
    "                # Check metadata\n",
    "                for ma in ['source_id', 'grid_label', 'table_id', 'variant_label']:\n",
    "                    assert ds.attrs[ma] == ds_sigma_reloaded.attrs[ma]\n",
    "\n",
    "            ##################### Verification plotting ##########################\n",
    "            print('plotting results')\n",
    "            try:\n",
    "                plot_omz_results(ds_sigma_reloaded)\n",
    "            except Exception as e:\n",
    "                print(f\"Plotting failed with: {e}\")\n",
    "            plt.show()\n",
    "            t1 = time.time()\n",
    "            print(f\"Time passed: {(t1-t0)/60 } minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/home/jbusecke/projects/cmip6_omz/data/processed/fine_density_tests_combined_2/none.none.MPI-ESM1-2-HR.none.r1i1p1f1.Omon.gn.none.none.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ~~CanESM5 crapped out (only for the variable o2 case)~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can I save the output?\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont execute this...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from multiprocessing.pool import ThreadPool\n",
    "# dask.config.set(pool=ThreadPool(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "foldername = \"fine_density_tests_combined_2\"\n",
    "tempfolder = pathlib.Path(f\"../../data/temp/scratch_temp/{foldername}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    xr.open_zarr(\n",
    "        tempfolder.joinpath(f\"temp_write_split_{str(i)}.zarr\"),\n",
    "        consolidated=False,\n",
    "        use_cftime=True\n",
    "    ) for i in range(251)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat(datasets, 'time', compat='override', coords='minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"fine_density_tests_combined_2\"\n",
    "# ofolder = maybe_create_folder(f\"../../data/external/{foldername}\")\n",
    "ofolder = pathlib.Path(f\"../../data/processed/{foldername}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_preprocessing.utils import cmip6_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_store = ofolder.joinpath(f\"{cmip6_dataset_id(ds)}_synthetic.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(manual_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_write_zarr(ds, manual_store, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_zarr(manual_store).thetao.isel(sigma_0=20).mean(['x', 'y']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to loop to write from one zarr to another? WTF is wrong with this machine?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_tigressdata-cmip6_omz]",
   "language": "python",
   "name": "conda-env-conda_tigressdata-cmip6_omz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
