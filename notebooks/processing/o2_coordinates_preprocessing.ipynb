{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe60ba99-22fd-4ed0-b9dc-aaa5e924b781",
   "metadata": {},
   "source": [
    "# Preprocessing for analyzing CMIP6 OMZ data in oxygen coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a226b8c4-feeb-4a52-890e-fc49cba27c49",
   "metadata": {},
   "source": [
    "## What is blocking this?\n",
    "\n",
    "- the new cmip6_pp masking would make this a lot easier (with the labels)\n",
    "\n",
    "\n",
    "## General notes\n",
    "- CM4 is not working due to the wonky chunks. Ill try to squeeze it through anyways, because I do not want to invest much more work here. This will all work better in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162d8450-0e36-4618-9064-48e3de8742bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_omz.upstream_stash import append_write_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86042e0c-99de-47ef-b4db-47b2e5b658ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhistogram.xarray import histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8279e757-6599-4c2d-971e-fe4ba01bf4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cmip6_preprocessing.utils import cmip6_dataset_id\n",
    "from cmip6_preprocessing.preprocessing import combined_preprocessing\n",
    "from cmip6_preprocessing.postprocessing import (\n",
    "    match_metrics,\n",
    "    interpolate_grid_label,\n",
    "    merge_variables,\n",
    "    concat_experiments,\n",
    ")\n",
    "from cmip6_preprocessing.drift_removal import match_and_remove_trend\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from cmip6_omz.utils import cmip6_collection, o2_models\n",
    "from cmip6_omz.upstream_stash import (\n",
    "    pick_first_member,\n",
    "    construct_static_dz\n",
    ")\n",
    "from cmip6_omz.units import convert_mol_m3_mymol_kg\n",
    "\n",
    "from xarrayutils.file_handling import maybe_create_folder\n",
    "\n",
    "### needs cleaning\n",
    "from cmip6_omz.omz_tools import omz_thickness_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf36c29-2469-4270-aefc-31d62b932d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x2aad89bf2340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "from multiprocessing.pool import ThreadPool\n",
    "dask.config.set(pool=ThreadPool(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a0c8f8-31a7-4f12-9609-c306740dd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up dask\n",
    "# from dask.distributed import LocalCluster, Client\n",
    "# mem_total = 128\n",
    "# workers = 2\n",
    "# threads = 4 # 4*6 seemed to work quite well, but I would like this to perform a bit better\n",
    "# cluster = LocalCluster(\n",
    "#     memory_limit=f\"{int(mem_total/workers)}GiB\",\n",
    "#     dashboard_address=9999,\n",
    "#     threads_per_worker=threads,\n",
    "#     n_workers = workers,\n",
    "#                       )\n",
    "# client = Client(cluster)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798cef8d-3dcc-412c-a662-d0b63c747dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs2/jbusecke/conda_tigressdata/envs/cmip6_omz/lib/python3.9/site-packages/xarrayutils/file_handling.py:118: UserWarning: Folder /projects/GEOCLIM/LRGROUP/jbusecke/projects_data/cmip6_depth_histogram does already exist.\n",
      "  warnings.warn(f\"Folder {path} does already exist.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ofolder = maybe_create_folder('/projects/GEOCLIM/LRGROUP/jbusecke/projects_data/cmip6_depth_histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9176d9d-f8a1-4046-87c6-6708a9100ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CanESM5-CanOE',\n",
       " 'CanESM5',\n",
       " 'CNRM-ESM2-1',\n",
       " 'ACCESS-ESM1-5',\n",
       " 'MPI-ESM-1-2-HAM',\n",
       " 'IPSL-CM6A-LR',\n",
       " 'MIROC-ES2L',\n",
       " 'UKESM1-0-LL',\n",
       " 'MPI-ESM1-2-HR',\n",
       " 'MPI-ESM1-2-LR',\n",
       " 'MRI-ESM2-0',\n",
       " 'NorCPM1',\n",
       " 'NorESM1-F',\n",
       " 'NorESM2-LM',\n",
       " 'NorESM2-MM',\n",
       " 'GFDL-CM4',\n",
       " 'GFDL-ESM4']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bbda19-a7ef-44b7-82a6-3f55c21940cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021.08.1\n"
     ]
    }
   ],
   "source": [
    "import dask.distributed\n",
    "print(dask.distributed.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f6faa7-0d58-47f6-90e9-f4554b165db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.version.zstore'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='48' class='' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [48/48 00:11<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.version.zstore'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 00:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if this does not work on jupyter.rc, we can add some logic to\n",
    "col = intake.open_esm_datastore(\n",
    "    cmip6_collection(zarr=True)\n",
    ")  # TODO: Check with nc files\n",
    "\n",
    "z_kwargs = {\"decode_times\": True, \"use_cftime\": True, \"consolidated\": True}\n",
    "n_kwargs = {\"decode_times\": True, \"use_cftime\": True, \"chunks\": {\"time\": 1}}\n",
    "\n",
    "variable_ids = [\"thetao\", \"so\", \"o2\", \"agessc\"]  # \"mlotst\"\n",
    "metric_variable_ids = [\"thkcello\", \"areacello\"]  # \"mlotst\"\n",
    "\n",
    "# models = o2_models()\n",
    "models = [\n",
    "#      'CanESM5-CanOE',\n",
    "#      'CanESM5',\n",
    "#      'CNRM-ESM2-1',\n",
    "#      'ACCESS-ESM1-5',\n",
    "#      'MPI-ESM-1-2-HAM',\n",
    "#      'IPSL-CM6A-LR',\n",
    "#      'MIROC-ES2L',\n",
    "#      'UKESM1-0-LL',\n",
    "     'MPI-ESM1-2-HR',\n",
    "#      'MPI-ESM1-2-LR',\n",
    "#      'MRI-ESM2-0',\n",
    "#      'NorCPM1',\n",
    "#      'NorESM1-F',\n",
    "#      'NorESM2-LM',\n",
    "#      'NorESM2-MM',\n",
    "#      'GFDL-CM4',\n",
    "#      'GFDL-ESM4'\n",
    "]\n",
    "# models = [m for m in o2_models() if 'GFDL' not in m]\n",
    "# models = ['ACCESS-ESM1-5']\n",
    "\n",
    "cat = col.search(\n",
    "    source_id=models,\n",
    "    grid_label=[\"gr\", \"gn\"],\n",
    "    experiment_id=[\"historical\", \"ssp585\"],\n",
    "    table_id=[\"Omon\"],\n",
    "    variable_id=variable_ids,\n",
    ")\n",
    "ds_dict = cat.to_dataset_dict(\n",
    "    aggregate=False,\n",
    "    zarr_kwargs=z_kwargs,\n",
    "    cdf_kwargs=n_kwargs,\n",
    "    preprocess=combined_preprocessing,\n",
    ")\n",
    "\n",
    "# make a separate metric dict to catch all possible metrics!\n",
    "cat_metrics = col.search(source_id=models, variable_id=metric_variable_ids)\n",
    "ds_metric_dict = cat_metrics.to_dataset_dict(\n",
    "    aggregate=False,\n",
    "    zarr_kwargs=z_kwargs,\n",
    "    cdf_kwargs=n_kwargs,\n",
    "    preprocess=combined_preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b99528-06b8-44ee-be8b-2cd2cf2f485f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_79429/2369097233.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b92ca6-af60-4a3d-a8b7-5c240a118f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "# new files (change in later and get rid of `load_trend_dict` (or refactor?) and `fix_trend_metadata`)\n",
    "# Load all trend files\n",
    "flist = list(pathlib.Path('../../data/external/cmip6_control_drifts/').absolute().glob('*.nc'))\n",
    "flist = [f for f in flist if any([v in str(f) for v in variable_ids])]\n",
    "trend_dict = {}\n",
    "for f in progress_bar(flist):\n",
    "    trend_dict[f.stem] = xr.open_mfdataset([f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98b6c1-8561-40aa-88ff-9ccaa462c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these ones are messed up...need a better way to deal with that in the previous step\n",
    "# see https://github.com/jbusecke/cmip6_preprocessing/issues/175\n",
    "incomplete_keys = ['CMIP.IPSL.IPSL-CM6A-LR.historical.r3i1p1f1.Omon.gn.none.area_o2']\n",
    "trend_dict = {k:ds for k,ds in trend_dict.items() if k not in incomplete_keys}\n",
    "\n",
    "ddict_tracers_detrended = match_and_remove_trend(\n",
    "    ds_dict,\n",
    "    trend_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7285ebb-258f-43b6-b5ed-ec1eb4ff5ea8",
   "metadata": {},
   "source": [
    "## Match metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444e989-7b70-4357-9112-d39ce2c6e7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one causes problems because the time is not as long as the full data...\n",
    "ddict_tracers_detrended_filtered = {\n",
    "    k: ds.squeeze()\n",
    "    for k, ds in ddict_tracers_detrended.items()\n",
    "    if not (\"CNRM-ESM2-1\" in k and \"r6i1p1f2\" in k)\n",
    "}\n",
    "\n",
    "ddict_matched = match_metrics(\n",
    "    ddict_tracers_detrended_filtered,\n",
    "    ds_metric_dict,\n",
    "    [\"areacello\", \"thkcello\"],\n",
    "    print_statistics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33366d92-c466-464a-8ec9-74cc5b2f6b5f",
   "metadata": {},
   "source": [
    "## Interpolate Grids and merge variables\n",
    "\n",
    "- handle the Norwegian Models inside `interpolate_grid_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d477b-d670-46d0-845f-3e6f35806eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"interpolate grids\\n\")\n",
    "ddict_matched_regrid = interpolate_grid_label(\n",
    "    ddict_matched, merge_kwargs={\"compat\": \"override\"}\n",
    ")  # This should be a default soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ffe70-b63e-4a5d-b5dc-c8531fac235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#patch the norwegian model in manually\n",
    "ddict_patch = merge_variables(ddict_matched)\n",
    "for name, ds in ddict_patch.items():\n",
    "    if 'Nor' in name and 'gr' in name:\n",
    "        patch_name = name.replace('.gr','')\n",
    "        ddict_matched_regrid[patch_name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8afd1b-772a-40d2-885a-f0ee86b785ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(list(ddict_matched_regrid.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcec50f1-5f96-448e-afa3-ce1df9c72e8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Concatenate experiments and pick the first full one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d79dc-9fda-4677-b87b-9045cd533c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# somehow xarray cannot deal with comparing list/int attrs (Occurs in CM4)\n",
    "# I should raise that, but lets fix it quickly here\n",
    "def clean_attrs(ds):\n",
    "    for a, attr in ds.attrs.items():\n",
    "        if isinstance(attr, int):\n",
    "            ds.attrs[a] = [attr]\n",
    "    return ds\n",
    "\n",
    "ddict_matched_regrid = {k:clean_attrs(ds) for k, ds in ddict_matched_regrid.items()}\n",
    "\n",
    "ddict_ex_combined = concat_experiments(\n",
    "    ddict_matched_regrid,\n",
    "    concat_kwargs={\n",
    "        'combine_attrs': 'drop_conflicts',\n",
    "        'compat': 'override',\n",
    "        'coords': 'minimal'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180fae3-9539-4adf-bd56-666af58112ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_ex_combined.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25285337-7509-4fc9-be12-de9973236eb0",
   "metadata": {},
   "source": [
    "Still need to deal with the access stuff here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74953707-72fc-43be-b506-1765b4594fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only pick full runs (historical and ssp585)\n",
    "ddict_ex_combined_full = {k:ds for k,ds in ddict_ex_combined.items() if len(ds.time)>3000}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03047dc-4660-46cd-a826-140f9a04b30a",
   "metadata": {},
   "source": [
    "## Check for problems and fix missing area/thickness manually\n",
    "\n",
    "This should be wrapped and brought upstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ecca6-ecb4-4462-8a6e-bcff402df3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_preprocessing.grids import combine_staggered_grid\n",
    "problems = {'missing_variables':[], 'missing_area':[], 'missing_thickness':[], 'reconstructed_area':[], 'reconstructed_thickness':[]}\n",
    "ddict_filtered = {}\n",
    "for name, ds in ddict_ex_combined_full.items():\n",
    "    flag = False\n",
    "    # Check that all necessary variables are given\n",
    "    missing_variables = [va for va in [\"thetao\", \"so\", \"o2\"] if va not in ds.variables]\n",
    "    if len(missing_variables)>0:\n",
    "        flag = True\n",
    "        problems['missing_variables'].append((name, missing_variables))\n",
    "        \n",
    "    # Check for area\n",
    "    if not 'areacello' in ds.coords:\n",
    "        if ds.attrs['grid_label'] == 'gr': # only reconstruct for regular grids\n",
    "            grid, ds = combine_staggered_grid(ds, recalculate_metrics=True)\n",
    "            # I am dropping dz_t here so it can be uniformly reconstructed\n",
    "            ds = ds.drop('dz_t')\n",
    "            ds = ds.assign_coords(areacello = (ds.dx_t * ds.dy_t).reset_coords(drop=True))\n",
    "            problems['reconstructed_area'].append(name)\n",
    "            assert 'areacello' in ds.coords\n",
    "        else:\n",
    "            flag = True\n",
    "            problems['missing_area'].append(name)\n",
    "    \n",
    "    # Check for thickness (and rename) TODO: We should probably not rename and just refactor to use `thkcello`\n",
    "    if \"thkcello\" in ds.coords:\n",
    "        ds = ds.rename({'thkcello': 'dz_t'})\n",
    "    else:\n",
    "        # try to reconstruct the thickness from static info\n",
    "        try:\n",
    "#             lev_vertices = cf_xarray.bounds_to_vertices(ds.lev_bounds, 'bnds').load()\n",
    "#             dz_t = lev_vertices.diff('lev_vertices')\n",
    "#             ds = ds.assign_coords(dz_t=('lev', dz_t.data))\n",
    "            ds = construct_static_dz(ds).rename({'thkcello': 'dz_t'})\n",
    "            problems['reconstructed_thickness'].append(name)\n",
    "        except Exception as e:\n",
    "            print(f'{name} thickness reconstruction failed with {e}')\n",
    "            print(ds)\n",
    "            problems['missing_thickness'].append(name)\n",
    "            flag=True\n",
    "            \n",
    "    if not flag:\n",
    "        ddict_filtered[name] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0db55-a400-4144-80f1-f7e71b615f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict_final = pick_first_member(ddict_filtered)#\n",
    "list(np.sort(list(ddict_final.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c634a16c-c880-473b-a463-484bcdd47818",
   "metadata": {},
   "source": [
    "## Prep Basin mask\n",
    "- Needs a separated Indian Ocean (Sam uses: 78E)\n",
    "- Refactor with the new masking using cf-xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9edbff-fbae-4d2b-a2af-8f77cc595607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ocean basin data\n",
    "import regionmask\n",
    "basins = regionmask.defined_regions.natural_earth.ocean_basins_50\n",
    "from cmip6_preprocessing.regionmask import merged_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f976ccc7-ac4b-4977-9efc-54b22fba1e60",
   "metadata": {},
   "source": [
    "## Define bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9f9a1-e7f7-46a2-88a4-f378d45219fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "o2_bins = np.hstack([-100, np.arange(5, 160, 5)])  # in mymol/kg\n",
    "o2_bins_converted = bins_converted = (\n",
    "    o2_bins / convert_mol_m3_mymol_kg(xr.DataArray([1])).data\n",
    ")\n",
    "\n",
    "# define mask bins\n",
    "mask_bins = np.arange(-0.5, 13.0, 1)# for now manual, but maybe there is a clever way to do this?\n",
    "mask_bins\n",
    "\n",
    "lat_bins = np.arange(-60, 91, 20)\n",
    "lat_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e2483-7c3b-435b-8c48-e3b4cb85b311",
   "metadata": {},
   "source": [
    "## This needs to go within the loop later\n",
    "\n",
    "\n",
    "- I need to also make sure nans are appropriately masked between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f48ef-f0fd-40e2-b653-78ee0bdf2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_exist(path):\n",
    "#     return path.exists()\n",
    "# hack for CM4\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a15b11-4471-4552-a670-927c7e5794a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5595967-5d89-4277-99bd-19ed75cffb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in ddict_final.items():\n",
    "#     ds = ds.chunk({'time':1, 'x':720, 'y':576})\n",
    "    ds = ds.reset_coords('dz_t')\n",
    "    mask = merged_mask(basins, ds)\n",
    "    mask.name = \"basin_mask\"\n",
    "\n",
    "    # Create a dataset\n",
    "    vol = ds.dz_t * ds.areacello\n",
    "\n",
    "    count = histogram(\n",
    "        ds.o2,\n",
    "        ds.lat,\n",
    "        mask,\n",
    "        bins=[o2_bins_converted, lat_bins, mask_bins],\n",
    "        dim=[\"x\", \"y\"],\n",
    "    )\n",
    "    volume = histogram(\n",
    "        ds.o2,\n",
    "        ds.lat,\n",
    "        mask,\n",
    "        bins=[o2_bins_converted, lat_bins, mask_bins],\n",
    "        weights=vol,\n",
    "        dim=[\"x\", \"y\"],\n",
    "    )\n",
    "    tracers = {}\n",
    "    for tr in ds.data_vars:\n",
    "        tracers[tr] = histogram(\n",
    "            ds.o2,\n",
    "            ds.lat,\n",
    "            mask,\n",
    "            bins=[o2_bins_converted, lat_bins, mask_bins],\n",
    "            weights=ds[tr] * vol,\n",
    "            dim=[\"x\", \"y\"],\n",
    "        )\n",
    "    ds_hist = xr.Dataset(dict(count=count, volume=volume, **tracers))\n",
    "    ds_hist.attrs = {k:v for k,v in ds.attrs.items() if k not in ['intake_esm_varname']}\n",
    "    print(f\"{ds_hist.nbytes/1e9} GB\")\n",
    "    path = ofolder.joinpath(f\"{cmip6_dataset_id(ds_hist)}\")\n",
    "    if not check_exist(path):\n",
    "        print(path)\n",
    "#         append_write_zarr(ds_hist, path, 60)\n",
    "        append_write_zarr(ds_hist, path, 1) # just for CM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5cd92-9274-432d-b9c1-d3e5af8c5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d9c77-f0fb-4524-8a49-fd4b7eb67ec8",
   "metadata": {},
   "source": [
    "## Test the output against something I know\n",
    "\n",
    "This seems fine. The bin count is not exact, but that is probably due to some changes in the numerical precision in one of the methods.\n",
    "\n",
    "Nice the mean tracer values also line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9375f-9426-4f35-8bee-35a31e1fa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to rename a few here so that it works and the dataset should probably be synth \n",
    "# Then I can move this to the tests.\n",
    "\n",
    "# define mask bins\n",
    "mask_bins = np.arange(-0.5, 13.0, 1)# for now manual, but maybe there is a clever way to do this?\n",
    "mask_bins\n",
    "\n",
    "lat_bins = np.arange(-90, 91, 20)\n",
    "lat_bins\n",
    "\n",
    "lev_bins = np.arange(0, 7000, 500)\n",
    "lev_bins\n",
    "vol = (ds.dz_t*ds.areacello) \n",
    "\n",
    "count = histogram(ds.o2, ds.lat, mask, bins=[o2_bins_converted, lat_bins, mask_bins], dim=['x','y'])\n",
    "volume = histogram(ds.o2, ds.lat, mask, bins=[o2_bins_converted, lat_bins, mask_bins], weights=vol, dim=['x','y'])\n",
    "tracers = {}\n",
    "for tr in ds.data_vars:\n",
    "    tracers[tr] = histogram(ds.o2, ds.lat, mask, bins=[o2_bins_converted, lat_bins, mask_bins], weights=ds[tr]*vol, dim=['x','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07c326-7aaf-4b19-9a93-1e8f86e953e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_omz.omz_tools import mask_basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c930762-b15f-4609-83e1-f34bf79387f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected_threshold = 0.082\n",
    "cutoff = o2_bins_converted[-1]\n",
    "test_full_pacific = test\n",
    "# expected_full_pacific = mask_basin(ds.o2.isel(time=0), drop=False)\n",
    "expected_full_pacific = ds.o2\n",
    "expected_full_pacific = xr.ones_like(expected_full_pacific).where(expected_full_pacific<=cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03dcfbc-69ae-42c8-a612-814f86722d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92399f-c07e-44f5-8577-4f2228da239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_full_pacific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411b090-05ae-4072-893d-6eb9e30a85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.o2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8936f46-64c2-48e2-9137-749ed7d1dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_full_pacific.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ff17c-b247-45c9-b55c-3dcdfa8f94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_pacific.sum().data/expected_full_pacific.sum().load().data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eee385-1717-4888-a187-18b0e673eb19",
   "metadata": {},
   "source": [
    "I suspect this is due to the numerial precision going wrong somewhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0700185-d0ae-4070-82ca-7e1869ad5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051748d1-5f7c-4db6-9084-1068fd23729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with masked basin\n",
    "cutoff = o2_bins_converted[-3]\n",
    "test_full_pacific = test.sel(basin_mask_bin=slice(1.5, 3.5))\n",
    "expected_full_pacific = mask_basin(ds.o2, drop=False)\n",
    "expected_full_pacific = xr.ones_like(expected_full_pacific).where(expected_full_pacific<=cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1507b-22c2-49ab-b133-91d27c763777",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_pacific.sum().data/expected_full_pacific.sum().load().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eae9eb-fa53-4b9f-bf41-0a6dd8d69125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with masked basin\n",
    "cutoff = o2_bins_converted[-3]\n",
    "test_full_pacific = test3.sel(basin_mask_bin=slice(1.5, 3.5)).sum()/test2.sel(basin_mask_bin=slice(1.5, 3.5)).sum()\n",
    "expected_full_pacific = mask_basin(ds, drop=False)\n",
    "expected_full_pacific = expected_full_pacific.where(expected_full_pacific<=cutoff)\n",
    "expected_full_pacific = expected_full_pacific.o2.weighted((expected_full_pacific.areacello*expected_full_pacific.dz_t).fillna(0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007019d-7d4c-4283-935d-b3e312afe0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_full_pacific.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eba2b3-a8a1-4299-b88b-75b3adb848fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full_pacific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35debebb-aa7e-483b-9253-4a9a5aa65340",
   "metadata": {},
   "source": [
    "## synthetic example for xhistogram\n",
    "\n",
    "I somehow cannot bin over 'lev'...ok for now. Problem is described in detail [here](https://github.com/xgcm/xhistogram/issues/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff0590-27c8-42dd-bcda-24ce55c51256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I cant get the count numbers to line up. What am I doing wrong here?\n",
    "da = xr.DataArray(np.random.rand(400,76), name='test')\n",
    "cutoff = 0.4\n",
    "hist = histogram(da, bins=np.array([-1e3, cutoff]))\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5dabab-fa5a-428b-af1d-2363c9efdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.ones_like(da).where(da<=cutoff).sum()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84f07a-a9de-48e9-8f71-8b8da94480b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f318a-b5ad-4ca5-b8f7-2b67f350afa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda_tigressdata-cmip6_omz]",
   "language": "python",
   "name": "conda-env-conda_tigressdata-cmip6_omz-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
