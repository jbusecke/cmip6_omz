{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "import intake\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fastjmd95 import rho\n",
    "import fastjmd95\n",
    "assert fastjmd95.__version__ >= \"0.2.1\"\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "from xarrayutils.file_handling import (\n",
    "    write,\n",
    "    maybe_create_folder,\n",
    "    file_exist_check,\n",
    "    temp_write_split,\n",
    ")\n",
    "from xarrayutils.utils import (\n",
    "    remove_bottom_values,\n",
    "    #mask_mixedlayer\n",
    ")\n",
    "from cmip6_preprocessing.preprocessing import (\n",
    "    combined_preprocessing\n",
    ")\n",
    "from cmip6_preprocessing.drift_removal import (\n",
    "    remove_trend,\n",
    "    match_and_remove_trend\n",
    ")\n",
    "from cmip6_preprocessing.utils import (\n",
    "    cmip6_dataset_id\n",
    ")\n",
    "\n",
    "from cmip6_preprocessing.postprocessing import (\n",
    "    combine_datasets,\n",
    "    match_metrics,\n",
    "    merge_variables\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from cmip6_omz.upstream_stash import (\n",
    "    xgcm_transform_wrapper as transform_wrapper,\n",
    "    match_and_detrend\n",
    ")\n",
    "from cmip6_omz.omz_tools import (\n",
    "    volume_consistency_checks,\n",
    "    omz_thickness,  \n",
    ")\n",
    "\n",
    "from cmip6_omz.utils import (\n",
    "    cmip6_collection,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#still depending on parallel repo for a few things here,\n",
    "#all of these only show up in the last step\n",
    "\n",
    "#from aguadv_omz_busecke_2021.utils import print_html\n",
    "from aguadv_omz_busecke_2021.preprocessing import (\n",
    "    save_and_reload_rechunker,\n",
    "    strip_encoding,\n",
    ")\n",
    "from aguadv_omz_busecke_2021.omz_tools import (\n",
    "    plot_omz_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This could go upstream in a more general form\n",
    "## but for now let's keep it here and readable\n",
    "def transform_wrapper_again(\n",
    "    ds_in,\n",
    "    intensive_vars=[\n",
    "        \"thetao\",\n",
    "        \"o2\",\n",
    "        \"so\",\n",
    "        \"agessc\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    #sigma_bins = np.array([0, 24.5, 26.5, 27.65, 100])\n",
    "    sigma_bins = np.array([0, 23.0, 24.5, 25.5, 26.5, 26.65, 26.7, 27.4, 27.65, 27.8, 100])\n",
    "    # define variables to be averaged (intensive quantities)\n",
    "    intensive_vars = [\n",
    "        \"thetao\",\n",
    "        \"o2\",\n",
    "        \"so\",\n",
    "        \"agessc\",\n",
    "    ]  # add 'uo', 'agessc' etc?\n",
    "\n",
    "    intensive_vars = [v for v in intensive_vars if v in ds_in.data_vars]\n",
    "\n",
    "    for iv in intensive_vars:\n",
    "        dz = (xr.ones_like(ds_in[iv]) * ds_in.dz_t).where(~np.isnan(ds_in[iv]))\n",
    "        ds_in[iv] = ds_in[iv] * dz\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        ds_out = transform_wrapper(\n",
    "            ds_in,\n",
    "            extensive_vars=[\"omz_thickness\"] + intensive_vars,\n",
    "            target=sigma_bins,\n",
    "        )\n",
    "\n",
    "    # reconvert the same variables\n",
    "    dz = ds_out.dz_t\n",
    "    for iv in intensive_vars:\n",
    "        ds_out[iv] = ds_out[iv] / dz\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function should go to upstream_stash\n",
    "def load_trend_dict(verbose = False):\n",
    "    \n",
    "    path_jb = '/tigress/GEOCLIM/LRGROUP/jbusecke/projects/aguadv_omz_busecke_2021/data/processed/linear_regression_time_zarr_multimember'\n",
    "    trendfolder = pathlib.Path(path_jb)\n",
    "    trend_models = np.unique([ds.attrs['source_id'] for ds in ds_dict.values()])\n",
    "    flist = []\n",
    "    for tm in trend_models:\n",
    "        flist = flist + list(trendfolder.glob(f'*{tm}*_trend.nc'))\n",
    "    \n",
    "    total = len(flist)\n",
    "    progress = progress_bar(range(total))\n",
    "    \n",
    "    trend_dict = {}\n",
    "    for i,path in enumerate(flist):\n",
    "        key = path.stem\n",
    "        ds = xr.open_mfdataset([path])\n",
    "        # write the filename in the dataset\n",
    "        ds.attrs.update({'filepath':str(path)})\n",
    "        # exclude all nan slopes (why are these there in the first place?)\n",
    "        if not np.isnan(ds.slope).all():\n",
    "            trend_dict[key] = ds\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"found all nan regression data for {path}\")\n",
    "        progress.update(i)\n",
    "    progress.update(total)\n",
    "    \n",
    "    return trend_dict\n",
    "\n",
    "\n",
    "#These are fixes so that the trend data works with cmip6_pp match_and_remove_trend\n",
    "#these issues should be addressed in the next iteration of trend file production\n",
    "def fix_trend_metadata(trend_dict):\n",
    "    for name, ds in trend_dict.items():\n",
    "        #restore attributes to trend datasets using file names\n",
    "        fn = (ds.attrs['filepath']).rsplit(\"/\")[-1]\n",
    "        fn_parse = fn.split('_')\n",
    "        ds.attrs['source_id'] = fn_parse[2]\n",
    "        ds.attrs['grid_label'] = fn_parse[5]\n",
    "        ds.attrs['experiment_id'] = fn_parse[3]\n",
    "        ds.attrs['table_id'] = fn_parse[4]\n",
    "        ds.attrs['variant_label'] = fn_parse[7]\n",
    "        ds.attrs['variable_id'] = fn_parse[8]\n",
    "        \n",
    "        #rename 'slope' variable to variable_id\n",
    "        if \"slope\" in ds.variables:\n",
    "            ds = ds.rename({\"slope\":ds.attrs[\"variable_id\"]})\n",
    "        \n",
    "        #error was triggered in line 350 of cmip6_preprocessing.drift_removal\n",
    "        ##this is a temporary workaround, and the one part of this function that might\n",
    "        ##require an upstream fix (though it might just be an environment issue)\n",
    "        ds = ds.drop('trend_time_range')\n",
    "        \n",
    "        trend_dict[name] = ds\n",
    "        \n",
    "    return trend_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_yearly(ds_in, freq=\"1AS\"):\n",
    "    # this drops some coordinates, so i need to convert them to data_vars and then reconvert\n",
    "    time_coords = [\n",
    "        co\n",
    "        for co in list(ds_in.coords)\n",
    "        if \"time\" in ds_in[co].dims and co not in [\"time\", \"time_bounds\"]\n",
    "    ]\n",
    "    #ds_out = ds_in.reset_coords(time_coords).resample(time=freq).mean()\n",
    "    ds_out = ds_in.reset_coords(time_coords).coarsen(time=12).mean()\n",
    "    ds_out = ds_out.assign_coords({co: ds_out[co] for co in time_coords})\n",
    "    ds_out.attrs.update({k: v for k, v in ds_in.attrs.items() if k not in [\"table_id\"]})\n",
    "    return ds_out\n",
    "\n",
    "\n",
    "def align_missing(ds_in):\n",
    "    \"\"\"Make sure that nans in all fields of a dataset are consistent.\n",
    "    Requires\"\"\"\n",
    "    # Due to the interpolation between `gr` and `gn`, we have to make sure that all data variables are masked in the same way!\n",
    "\n",
    "    ds_mask = ds_in\n",
    "    # for generalization np.logical_or.reduce((x, y, z))https://stackoverflow.com/questions/20528328/numpy-logical-or-for-more-than-two-arguments\n",
    "    combo_nanmask = np.logical_or(\n",
    "        np.isnan(ds_mask.o2).all(\"time\").load(),\n",
    "        np.isnan(ds_mask.sigma_0).all(\"time\").load(), #originally thetao\n",
    "    )\n",
    "    try:\n",
    "        plt.figure()\n",
    "        combo_nanmask.isel(lev=5).plot()\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    ds_out = ds_in.where(~combo_nanmask)\n",
    "\n",
    "    #if 'areacello' in ds_in.data_vars:\n",
    "    #    ds_out['areacello'] = ds_in['areacello']\n",
    "    \n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_wrapper(ds_in):\n",
    "    \n",
    "    # fix the attribute parsed by intake-esm\n",
    "    for k,v in ds_in.attrs.items():\n",
    "        if v is None:\n",
    "            print(f\"Replacing {k} attrs value with `none`\")\n",
    "            ds_in.attrs[k] = 'none'\n",
    "            \n",
    "    ds_in = strip_encoding(ds_in)\n",
    "    \n",
    "    if 'member_id' in ds_in.dims:\n",
    "        if isinstance(ds_in.member_id.data, object):\n",
    "            ds_in['member_id'] = ds_in['member_id'].astype(str)\n",
    "\n",
    "    # strip all the coords to avoid trouble\n",
    "    delete_coords = [\n",
    "        \"branch_time_in_parent\",\n",
    "        \"branch_time_in_child\",\n",
    "        \"parent_time_units\",\n",
    "        \"child_time_units\",\n",
    "        \"parent_variant_label\",  # these are all scalar coords (and zarr doesnt like those?)\n",
    "        \"time_bounds\",  # this is a bit different one, but makes trouble as a coordinate?\n",
    "    ]\n",
    "\n",
    "    ds_out = ds_in.drop([co for co in ds_in.coords if co in delete_coords])\n",
    "\n",
    "    # see below. Make a new fake 'outer' coordinate. I think the values really dont matter?\n",
    "    ds_out = ds_out.assign_coords(\n",
    "        lev_outer=np.hstack(\n",
    "            [0, (ds_out.lev.data[1:] + ds_out.lev.data[0:-1]) / 2, 5e10]\n",
    "        )\n",
    "    )\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mixed_layer_wrapper(ds_in):\n",
    "#     if \"mlotst\" in ds_in.data_vars:\n",
    "#         ds_out = mask_mixedlayer(ds_in, ds_in.mlotst)\n",
    "#         ds_out = ds_out.drop_vars(\"mlotst\")\n",
    "        \n",
    "#         ds_out.attrs[\"mixed_layer_removed\"] = \"true\"\n",
    "#     else:\n",
    "#         print(f\"No mixed layer values found for ...\")\n",
    "#         ds_out.attrs[\"mixed_layer_removed\"] = \"false\"\n",
    "#     return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function filters out any incomplete members\n",
    "\n",
    "# def filter_incomplete_members(ds_dict, \n",
    "#                               nec_vars = ['thetao', 'so', 'o2', 'agessc']\n",
    "#                              ):\n",
    "    \n",
    "#     #collect metadata and make a dataframe\n",
    "#     metadata = {\n",
    "#         'experiment' : [],\n",
    "#         'variable': [],\n",
    "#         'variant': [],\n",
    "#         'version': []\n",
    "#     }\n",
    "#     for name, ds in ds_dict.items():\n",
    "#         metadata['experiment'].append(ds.attrs['experiment_id'])\n",
    "#         metadata['variable'].append(ds.attrs['variable_id'])\n",
    "#         metadata['variant'].append(ds.attrs['variant_label'])\n",
    "#         metadata['version'].append(int(ds.attrs['version_id'].replace('v','')))\n",
    "#     metadata = pd.DataFrame(metadata)\n",
    "    \n",
    "#     experiments = metadata['experiment'].unique()\n",
    "#     metadata_ = metadata.groupby('experiment')\n",
    "#     mem_full = {}\n",
    "#     for exp in experiments:\n",
    "#         meta = metadata_.get_group(exp)\n",
    "#         meta_ = meta.groupby('variant')\n",
    "#         variants = meta['variant'].unique()\n",
    "#         mem_full[exp] = []\n",
    "#         for mem in variants:\n",
    "#             if all(var in list(meta_.get_group(mem)['variable'])\n",
    "#                    for var in nec_vars):\n",
    "#                 mem_full[exp].append(mem)\n",
    "#             #make sure there are no duplicates\n",
    "#             assert len(list(meta_.get_group(mem)['variable'])) == len(set(list(meta_.get_group(mem)['variable'])))\n",
    "        \n",
    "#     ds_dict = {k:v for k,v in ds_dict.items() if v.attrs['variant_label'] in mem_full[v.attrs['experiment_id']]}\n",
    "#     ds_dict = {k:v for k,v in ds_dict.items() if 'rho' not in v.dims}\n",
    "    \n",
    "#     return ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local convenience functions for final cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_variables(ds, name, required_vars):\n",
    "    \n",
    "    if not all(\n",
    "        [va in ds.variables for va in required_vars]\n",
    "    ):\n",
    "        print(f\"!!!\\nMissing Variables {name}\\n!!!\")\n",
    "        print([va for va in list(ds.variables) if va in required_vars])\n",
    "        \n",
    "        check = True\n",
    "    else:\n",
    "        print(f\"@@@@@@@@@@@@@@@@@\\nProcessing {name}\\n@@@@@@@@@@@@@@@@@\")\n",
    "        check = False\n",
    "    return check\n",
    "    \n",
    "    \n",
    "def is_zarr(fn):\n",
    "    extension = fn.split('.')[-1]\n",
    "    if extension == 'nc':\n",
    "        is_zarr = False\n",
    "    elif extension == 'zarr':\n",
    "        is_zarr = True\n",
    "    else:\n",
    "        raise RuntimeError('Unrecognized File Extension')\n",
    "    return is_zarr\n",
    "\n",
    "def reload_preexisting(filename, overwrite = True):\n",
    "    print(\"Skipping. File exists already.\")\n",
    "    if is_zarr(filename):\n",
    "        ds_sigma_reloaded = xr.open_zarr(\n",
    "            filename, use_cftime=True, consolidated=True\n",
    "        )\n",
    "    else:\n",
    "        ds_sigma_reloaded = xr.open_dataset(\n",
    "            filename, use_cftime = True\n",
    "        )\n",
    "        try:\n",
    "            plot_omz_results(ds_sigma_reloaded)\n",
    "        except Exception as e:\n",
    "            print(f\"Plotting failed with: {e}\")\n",
    "    return ds_sigma_reloaded\n",
    "\n",
    "def save_and_reload(ds, tempfolder):\n",
    "    print('First temp save')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        ds, tempfilelist_var = temp_write_split(\n",
    "            ds,\n",
    "            tempfolder,\n",
    "            method='dimension',\n",
    "            verbose=True,\n",
    "            split_interval=12*10 if len(ds.x) < 700 else 12*1, # 2 years worked fine with ESM4 but for CM4 I just reduced it again..\n",
    "            file_name_pattern='raw_intermediate'\n",
    "        )\n",
    "        tempfilelist.extend(tempfilelist_var)\n",
    "        \n",
    "        print(\"Rechunking incoming dataset\")\n",
    "        with ProgressBar():\n",
    "            temppath = tempfolder.joinpath(\"rechunked_intermediate.zarr\")\n",
    "            ds = save_and_reload_rechunker(ds.astype(np.float32), path=temppath)\n",
    "            tempfilelist.append(temppath)\n",
    "        return ds, tempfilelist\n",
    "    \n",
    "def vol_consistency_check_wrapper(ds, ds_sigma):\n",
    "    perc_difference, omz_perc_difference = volume_consistency_checks(\n",
    "        ds, ds_sigma\n",
    "    )\n",
    "    print(\n",
    "        f\"Relative difference ocean vol: {abs(perc_difference).data}% | OMZ vol {abs(omz_perc_difference).data}%\"\n",
    "        )\n",
    "    \n",
    "    if (abs(perc_difference) > 0.1).any() or (\n",
    "        abs(omz_perc_difference) > 0.25# Had to increase for ESM4 before most would pass 0.01\n",
    "        ).any():\n",
    "        print(\"Volume differences exceed threshold. NOT SAVING.\")\n",
    "        print('\\x1b[31m\"red\"\\x1b[0m')\n",
    "        consistent = False\n",
    "    else:\n",
    "        consistent = True\n",
    "    return consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samjd/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/xarrayutils/file_handling.py:118: UserWarning: Folder ../../data/processed/density_remapped_annual does already exist.\n",
      "  warnings.warn(f\"Folder {path} does already exist.\", UserWarning)\n",
      "/home/samjd/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/xarrayutils/file_handling.py:118: UserWarning: Folder ../../data/temp/scratch_temp/density_remapped_annual does already exist.\n",
      "  warnings.warn(f\"Folder {path} does already exist.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "foldername = \"density_remapped_annual\"\n",
    "ofolder = maybe_create_folder(f\"../../data/processed/{foldername}\")\n",
    "tempfolder = maybe_create_folder(f\"../../data/temp/scratch_temp/{foldername}\")\n",
    "\n",
    "# global parameters\n",
    "o2_bins = np.array([10, 40, 60, 80, 100, 120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CMCC-CM2-HR4', 'EC-Earth3P-HR', 'HadGEM3-GC31-MM',\n",
       "       'HadGEM3-GC31-HM', 'HadGEM3-GC31-LM', 'EC-Earth3P', 'ECMWF-IFS-HR',\n",
       "       'ECMWF-IFS-LR', 'HadGEM3-GC31-LL', 'CMCC-CM2-VHR4', 'GFDL-CM4',\n",
       "       'GFDL-AM4', 'IPSL-CM6A-LR', 'E3SM-1-0', 'CNRM-CM6-1', 'GFDL-ESM4',\n",
       "       'GFDL-CM4C192', 'GFDL-OM4p5B', 'GFDL-ESM2M', 'GISS-E2-1-G',\n",
       "       'GISS-E2-1-H', 'CNRM-ESM2-1', 'BCC-CSM2-MR', 'BCC-ESM1', 'MIROC6',\n",
       "       'AWI-CM-1-1-MR', 'EC-Earth3-LR', 'IPSL-CM6A-ATM-HR', 'CESM2',\n",
       "       'CESM2-WACCM', 'CNRM-CM6-1-HR', 'MRI-ESM2-0', 'CanESM5',\n",
       "       'SAM0-UNICON', 'GISS-E2-1-G-CC', 'UKESM1-0-LL', 'EC-Earth3',\n",
       "       'EC-Earth3-Veg', 'FGOALS-f3-L', 'CanESM5-CanOE', 'INM-CM4-8',\n",
       "       'INM-CM5-0', 'NESM3', 'MPI-ESM-1-2-HAM', 'CAMS-CSM1-0',\n",
       "       'MPI-ESM1-2-LR', 'MPI-ESM1-2-HR', 'MRI-AGCM3-2-H', 'MRI-AGCM3-2-S',\n",
       "       'MCM-UA-1-0', 'INM-CM5-H', 'KACE-1-0-G', 'NorESM2-LM',\n",
       "       'FGOALS-f3-H', 'FGOALS-g3', 'MIROC-ES2L', 'FIO-ESM-2-0', 'NorCPM1',\n",
       "       'NorESM1-F', 'MPI-ESM1-2-XR', 'CESM1-1-CAM5-CMIP5', 'E3SM-1-1',\n",
       "       'KIOST-ESM', 'ACCESS-CM2', 'NorESM2-MM', 'ACCESS-ESM1-5',\n",
       "       'CESM2-FV2', 'GISS-E2-2-G', 'CESM2-WACCM-FV2', 'IITM-ESM', 'CIESM',\n",
       "       'E3SM-1-1-ECA', 'TaiESM1', 'AWI-ESM-1-1-LR', 'EC-Earth3-Veg-LR',\n",
       "       'CMCC-ESM2', 'CAS-ESM2-0', 'CMCC-CM2-SR5', 'EC-Earth3-AerChem',\n",
       "       'IPSL-CM5A2-INCA', 'BCC-CSM2-HR', 'EC-Earth3P-VHR',\n",
       "       'CESM1-WACCM-SC', 'EC-Earth3-CC', 'IPSL-CM6A-LR-INCA',\n",
       "       'MIROC-ES2H'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using google storage catalog\n",
    "#but I could probably use tigress if path is /tigress/GEOCLIM/...\n",
    "#path = '/tigress/GEOCLIM/LRGROUP/jbusecke/projects/cmip_data_management_princeton/catalogs/tigressdata-cmip6.json'\n",
    "path = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(path)\n",
    "col.df['source_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.member_id.table_id.variable_id.grid_label.zstore.dcpp_init_year.version'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We are replacing load_single_datasets wrapper from old pipeline\n",
    "#with some unwrapped commands...\n",
    "\n",
    "z_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True}\n",
    "variable_ids = [\"thetao\", \"so\", \"o2\", \"agessc\", \"thkcello\", \"areacello\"] #\"mlotst\"\n",
    "\n",
    "cat = col.search(\n",
    "    source_id=[\"CanESM5\"],\n",
    "    grid_label=[\"gr\", \"gn\"],\n",
    "    experiment_id=[\"historical\"],\n",
    "    table_id=[\"Omon\", \"Ofx\"],\n",
    "    variable_id=variable_ids,\n",
    "    member_id = [\"r1i1p2f1\"]\n",
    ")\n",
    "\n",
    "\n",
    "ds_dict = cat.to_dataset_dict(\n",
    "        aggregate=False,\n",
    "        zarr_kwargs=z_kwargs,\n",
    "        preprocess=combined_preprocessing,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='155' class='' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [155/155 01:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trend_dict = load_trend_dict()\n",
    "\n",
    "#These are fixes so that the trend data works with cmip6_pp match_and_remove_trend\n",
    "#these issues should be addressed in the next iteration of trend file production\n",
    "trend_dict = fix_trend_metadata(trend_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_vars = ['areacello', 'thkcello']\n",
    "ddict_tracers = {name:ds for name, ds in ds_dict.items() if ds.attrs['variable_id'] not in metric_vars}\n",
    "ddict_metrics = {name:ds for name, ds in ds_dict.items() if ds.attrs['variable_id'] in metric_vars}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    ddict_tracers_detrended = match_and_remove_trend(\n",
    "        ddict_tracers,\n",
    "        trend_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See if files are actually detrended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detrended_names = []\n",
    "# for name, ds in ddict_detrended.items():\n",
    "#     if ds.attrs['variable_id'] == 'o2':\n",
    "#         ds_detrnd = ddict_detrended[name]\n",
    "#         ds_orig = ds_dict[name]\n",
    "#         break\n",
    "           \n",
    "# orig_o2 = np.nanmean(ds_orig.o2.isel(x = 100, y = 220).data, axis = 1)[-200:] #just look at last 200 months\n",
    "# detrnd_o2 = np.nanmean(ds_detrnd.o2.isel(x = 100, y = 220).data, axis = 1)[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2,figsize = (15, 4))\n",
    "# ax[0].plot(np.arange(len(orig_o2)), orig_o2, alpha = 0.5, label = 'original')\n",
    "# ax[0].plot(np.arange(len(detrnd_o2)), detrnd_o2, alpha = 0.5, label = 'detrended')\n",
    "\n",
    "# ax[1].plot(np.arange(len(orig_o2)), orig_o2 - detrnd_o2, label = 'recovered slope')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement combining grid labels step here when ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rechunk to make sure all chunks are even\n",
    "# for name, ds in ddict_detrended.items():\n",
    "#     if 'time' in ds.dims:\n",
    "#         ddict_detrended[name] = ds.chunk(chunks={'time':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I need to rework this so that it works on the nested datasets later in the pipeline\n",
    "#ddict_detrended_filtered = filter_incomplete_members(ddict_detrended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching metrics\n",
      "\n",
      "nesting datasets for each member \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('matching metrics\\n')\n",
    "ddict_matched = match_metrics(ddict_tracers_detrended, ddict_metrics, ['areacello', 'thkcello'])\n",
    "\n",
    "print('nesting datasets for each member \\n')\n",
    "ddict_nested = merge_variables(ddict_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter incomplete datasets now\n",
    "nec_vars = [\"thetao\", \"so\", \"o2\", \"thkcello\", \"areacello\"]\n",
    "incomplete_mem = [k for k in ddict_nested \n",
    "                  if not all(elem in list(ddict_nested[k].variables) for elem in nec_vars)]\n",
    "for k in incomplete_mem: del ddict_nested[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, ds in ddict_nested.items():\n",
    "    ddict_nested[name] = ds.rename({'thkcello': 'dz_t'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@\n",
      "Processing CanESM5.gn.historical.Omon.r1i1p2f1\n",
      "@@@@@@@@@@@@@@@@@\n",
      "Writing to ../../data/processed/density_remapped_annual/CMIP.CCCma.CanESM5.historical.r1i1p2f1.Omon.gn.v20190429_r1i1p2f1.nc\n",
      "Replacing intake_esm_varname attrs value with `none`\n",
      "masking all fields consistently\n",
      "[                                        ] | 2% Completed |  3.8s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-173b0dafcb49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"masking all fields consistently\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Remove bottom values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_bottom_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-fc21aeb1e0fc>\u001b[0m in \u001b[0;36malign_missing\u001b[0;34m(ds_in)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# for generalization np.logical_or.reduce((x, y, z))https://stackoverflow.com/questions/20528328/numpy-logical-or-for-more-than-two-arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     combo_nanmask = np.logical_or(\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#originally thetao\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \"\"\"\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_temp_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;31m# evaluate all the dask arrays simultaneously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0mevaluated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlazy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                 \u001b[0mfire_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                         \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/site-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cmip6_omz2/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#The final loop to vertically transform to sigma-space and save output\n",
    "\n",
    "required_vars = [\"thetao\", \"so\", \"o2\", \"dz_t\"]# \"areacello\",\n",
    "\n",
    "for name, ds in ddict_nested.items():\n",
    "    #make sure all necessary data is present\n",
    "    if missing_variables(ds, name, required_vars):\n",
    "        continue\n",
    "    \n",
    "    else:    \n",
    "        dataset_id = f\"{cmip6_dataset_id(ds)}_{ds.attrs['variant_label']}\"\n",
    "        filename = ofolder.joinpath(f\"{dataset_id}.nc\")\n",
    "        t0 = time.time()\n",
    "        \n",
    "        if file_exist_check(filename) and not overwrite:\n",
    "            ds_sigma_reloaded = reload_preexisting(filename)\n",
    "        else:\n",
    "            print(f\"Writing to {filename}\")\n",
    "            tempfilelist = []\n",
    "            \n",
    "            #plt.figure()\n",
    "            #ds.isel(time=200, lev=10).thetao.plot()\n",
    "            #plt.show()\n",
    "\n",
    "            if ds.attrs[\"experiment_id\"] == \"piControl\":\n",
    "                ds = ds.isel(time=slice(-300 * 12, None))\n",
    "\n",
    "            ds = preprocessing_wrapper(ds)\n",
    "            \n",
    "            #ds, tempfilelist = save_and_reload(ds, tempfolder)\n",
    "\n",
    "            # I need to align.mask the thickness aswell!\n",
    "            ds = ds.reset_coords([\"dz_t\"])\n",
    "            ds[\"sigma_0\"] = (fastjmd95.rho(ds.so, ds.thetao, 0) - 1000)\n",
    "            \n",
    "            #perform nan-masking functions\n",
    "            print(\"masking all fields consistently\")\n",
    "            with ProgressBar():\n",
    "                ds = align_missing(ds)\n",
    "            print(f\"Remove bottom values\")\n",
    "            ds = remove_bottom_values(ds)\n",
    "            #print(f\"Remove ML\")\n",
    "            #ds = mixed_layer_wrapper(ds)\n",
    "\n",
    "            ds = ds.set_coords(\"dz_t\")\n",
    "\n",
    "            ds[\"omz_thickness\"] = omz_thickness(\n",
    "                ds, o2_bins=o2_bins\n",
    "            )\n",
    "            \n",
    "            ds_sigma_monthly = transform_wrapper_again(ds)\n",
    "\n",
    "            if vol_consistency_check_wrapper(ds, ds_sigma_monthly):\n",
    "                continue\n",
    "            else:\n",
    "                ds_sigma_yearly = strip_encoding(resample_yearly(ds_sigma_monthly))\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "                    ds_sigma_yearly, tempfilelist_var = temp_write_split(\n",
    "                        ds_sigma_yearly,\n",
    "                        tempfolder,\n",
    "                        verbose=True,\n",
    "                        method='dimension',\n",
    "                        split_interval=3,\n",
    "                    )\n",
    "                    tempfilelist.extend(tempfilelist_var)\n",
    "\n",
    "                # rechunk the resulting file for convenience\n",
    "                ds_sigma_yearly = strip_encoding(\n",
    "                    ds_sigma_yearly.chunk({\"x\": -1, \"y\": -1, \"time\": 2})\n",
    "                )\n",
    "\n",
    "                #################### write out results ########################\n",
    "                with ProgressBar():\n",
    "                    ds_sigma_reloaded = write(\n",
    "                        ds_sigma_yearly,\n",
    "                        filename,\n",
    "                        overwrite=True,\n",
    "                        force_load=False,\n",
    "                        check_zarr_complete=True,\n",
    "                    )\n",
    "            ###### delete temps ######\n",
    "            for tf in tempfilelist:\n",
    "                if tf.exists():\n",
    "                    shutil.rmtree(tf)\n",
    "            ##################### Verification plotting ##########################\n",
    "            try:\n",
    "                plot_omz_results(ds_sigma_reloaded)\n",
    "            except Exception as e:\n",
    "                print(f\"Plotting failed with: {e}\")\n",
    "        plt.show()\n",
    "        t1 = time.time()\n",
    "        print(f\"Time passed: {(t1-t0)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmip6_omz2",
   "language": "python",
   "name": "cmip6_omz2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
